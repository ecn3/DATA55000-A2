{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Assignment Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Decision Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our priors are defiend by a MLE of the Bernoulli distribution for each class of examples, with the parameter P:\n",
    "\\begin{equation}\n",
    "\\hat P = \\frac{{\\sum {{x^t}} }}{N}\n",
    "\\end{equation}\n",
    "\n",
    "Our liklihoods are defiend by a MLE of the Normal distribution, with the parameters mean and standard deviation:\n",
    "\n",
    "\\begin{equation}\n",
    "p(x|\\mu ,{\\sigma}) = \\frac{1}{{\\sqrt {2{\\sigma ^2}\\pi } }}\\exp \\left[ {\\frac{{{{(x - \\mu )}^2}}}{{2{\\sigma ^2}}}} \\right], - \\infty  < x < \\infty\n",
    "\\end{equation}\n",
    "\n",
    "We use the univariate normal density function provided in SciPy to calulate values or vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm.pdf(INPUT_SAMPLE_OR_VECTOR, loc=MEAN_VALUE, scale=STANDARD_DEVIATION_VALUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Bayes theorem to find a posterior probability:\n",
    "\\begin{equation}\n",
    "p({C_1|\\mathbf{x}}) = \\frac{p({\\mathbf{x}}|C_1)\\hat P(C_1)}{p({\\mathbf{x}}|C_1)\\hat P(C_1) + p({\\mathbf{x}}|C_2)\\hat P(C_2)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_1 = 0.3\n",
    "prior_2 = 1 - prior_1\n",
    "liklihood_1_mean = -1.5\n",
    "liklihood_1_std = 0.5\n",
    "liklihood_2_mean = 1\n",
    "liklihood_2_std = 0.8\n",
    "\n",
    "x_i = -0.4\n",
    "\n",
    "# normal density function: loc = mean, scale = standard deviation\n",
    "print \"Posterior 1: \", norm.pdf(x_i, loc=liklihood_1_mean, scale=liklihood_1_std)*prior_1 /  \\\n",
    "    (norm.pdf(x_i, loc=liklihood_1_mean, scale=liklihood_1_std)*prior_1+  \\\n",
    "    norm.pdf(x_i, loc=liklihood_2_mean, scale=liklihood_2_std)*prior_2)\n",
    "print \"Posterior 2: \", norm.pdf(x_i, loc=liklihood_2_mean, scale=liklihood_2_std)*prior_2 /  \\\n",
    "    (norm.pdf(x_i, loc=liklihood_1_mean, scale=liklihood_1_std)*prior_1+  \\\n",
    "    norm.pdf(x_i, loc=liklihood_2_mean, scale=liklihood_2_std)*prior_2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a set of uniformly distributed points for plotting\n",
    "x_min = min([norm.ppf(.00001, loc=liklihood_1_mean, scale=liklihood_1_std),\n",
    "     norm.ppf(.00001, loc=liklihood_2_mean, scale=liklihood_2_std)])\n",
    "x_max = max([norm.ppf(0.99999, loc=liklihood_1_mean, scale=liklihood_1_std),\n",
    "     norm.ppf(0.99999, loc=liklihood_2_mean, scale=liklihood_2_std)])\n",
    "x = np.linspace(x_min, x_max, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate over a range for plotting\n",
    "liklihood_1 = norm.pdf(x, loc=liklihood_1_mean, scale=liklihood_1_std)\n",
    "liklihood_2 = norm.pdf(x, loc=liklihood_2_mean, scale=liklihood_2_std)\n",
    "\n",
    "evidence = liklihood_1*prior_1+liklihood_2*prior_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.xlim([x_min,x_max])\n",
    "plt.ylim([0,1])\n",
    "plt.title('liklihoods')\n",
    "plt.plot(x, liklihood_1, \n",
    "         'r' ,lw=2, alpha=0.6, label='p(x|C_1)')\n",
    "plt.plot(x, liklihood_2, \n",
    "         'b', lw=2, alpha=0.6, label='p(x|C_2)')\n",
    "ax.legend(loc='best', frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.xlim([x_min,x_max])\n",
    "plt.ylim([0,1])\n",
    "plt.title('liklihoods * priors')\n",
    "plt.plot(x, liklihood_1*prior_1, \n",
    "         'r' ,lw=2, alpha=0.6, label='p(x|C_1)*P(C_1)')\n",
    "plt.plot(x, liklihood_2*prior_2, \n",
    "         'b', lw=2, alpha=0.6, label='p(x|C_2)*P(C_2)')\n",
    "ax.legend(loc='best', frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.xlim([x_min,x_max])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.title('posteriors')\n",
    "plt.plot(x, (liklihood_1*prior_1)/evidence, \n",
    "         'r' ,lw=2, alpha=0.6, label='P(C_1|x)')\n",
    "plt.plot(x, (liklihood_2*prior_2)/evidence, \n",
    "         'b', lw=2, alpha=0.6, label='P(C_2|x)')\n",
    "ax.legend(loc='best', frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn (Perceptron Revisited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mlclass import simplemetrics, plot_decision_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Fisher's Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we download Fisher's Iris dataset and import it as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/'\n",
    "        'machine-learning-databases/iris/iris.data', header=None)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Plot the Data\n",
    "\n",
    "We will only keep the classes 'setsosa' and 'versicolor'. \n",
    "\n",
    "We create a matrix of training samples:\n",
    "\\begin{equation}\n",
    "\\textbf{X} = \\left\\{ {{{\\mathbf{x}}^t}} \\right\\}_{t = 1}^N\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sepal length and petal length for samples 1:100 (Setsosa and Versicolor)\n",
    "X = df.iloc[0:100, [0, 1]].values\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "And a vector for the true classes.\n",
    "\\begin{equation}\n",
    "\\textbf{r} = \\left\\{ {{r^t}} \\right\\}_{t = 1}^N\n",
    "\\end{equation}\n",
    "\n",
    "We put setsosa = 0 and virginica = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select setosa and versicolor\n",
    "y = df.iloc[0:100, 4].values\n",
    "y = np.where(y == 'Iris-setosa', 0, 1)\n",
    "y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "plt.scatter(X[:50, 0], X[:50, 1],alpha=0.5, c='b', edgecolors='none', label='setosa %2s'%(y[0]))\n",
    "plt.scatter(X[50:100, 0], X[50:100, 1],alpha=0.5, c='r', edgecolors='none', label='versicolor %2s'%(y[50]))\n",
    "\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADALINE Model in 2D\n",
    "\n",
    "Then we instantiate a new Perceptron object, setup its parameters, and fit this model.\n",
    "\n",
    "\\begin{equation}\n",
    "g({\\mathbf{x}}^t|\\mathbf{w}) = \\phi({{\\mathbf{w}}^T}{{\\mathbf{x}}^t})\n",
    "\\end{equation}\n",
    "\n",
    "Our loss function is sum of squares.\n",
    "\n",
    "\\begin{equation}\n",
    "E({\\mathbf{w}}|{\\mathbf{X}}) = \\frac{1}{N}\\sum\\limits_{t = 1}^N {{{({r^t} - {y^t})}^2}}\n",
    "\\end{equation}\n",
    "\n",
    "The fit method finds the best parameters by computing the gradient using sum of squares loss, then making an adjustment in the correct direction for T iterations.\n",
    "\n",
    "\\begin{equation}\n",
    "{\\mathbf{w} ^*} = \\arg \\mathop {\\min }\\limits_\\mathbf{w}  E(\\mathbf{w} |{\\mathbf{X}})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "ppn = Perceptron(eta0=0.1,n_iter=1)\n",
    "ppn.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In 2D, we can see the decision regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply plot the model over a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_2d(ppn,X,y,title=\"Training Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model can now predict a new value for our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = ppn.predict(X)\n",
    "predicted[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use a function we provide to plot the confusion matrix and ROC curve. Note that in this case, setsosa is negative and versicolor is the positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Perceptron Error on Training Set\"\n",
    "simplemetrics(y,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is not a good model because we tested on the training set. We will see a better model below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "csv_data = '''A,B,C,D\n",
    "1.0,2.0,3.0,4.0\n",
    "5.0,6.0,,8.0\n",
    "10.0,11.0,12.0,'''\n",
    "\n",
    "# If you are using Python 2.7, you need\n",
    "# to convert the string to unicode:\n",
    "csv_data = unicode(csv_data)\n",
    "\n",
    "# typically we read a CSV file like this\n",
    "# df = pd.read_csv('example.csv')\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Typically we drop missing values like this:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is typically what you want to do:\n",
    "# remove samples with missing values\n",
    "df2 = df.dropna()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# however, you might sometimes want to drop columns with\n",
    "# missing values.\n",
    "df3 = df.dropna(axis=1)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only drop rows where all columns are NaN\n",
    "df4 = df.dropna(how='all')\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows that have not at least 4 non-NaN values\n",
    "df5 = df.dropna(thresh=4)\n",
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imr = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imr = imr.fit(df)\n",
    "imputed_data = imr.transform(df.values)\n",
    "imputed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([['green', 'M', 10.1, 'class1'],\n",
    "                   ['red', 'L', 13.5, 'class2'],\n",
    "                   ['blue', 'XL', 15.3, 'class1']])\n",
    "\n",
    "df.columns = ['color', 'size', 'price', 'classlabel']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Ordinal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_mapping = {'XL': 3,\n",
    "                'L': 2,\n",
    "                'M': 1}\n",
    "\n",
    "df['size'] = df['size'].map(size_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_size_mapping = {v: k for k, v in size_mapping.items()}\n",
    "df['size'].map(inv_size_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(df['classlabel']))}\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['classlabel'] = df['classlabel'].map(class_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "df['classlabel'] = df['classlabel'].map(inv_class_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class_le = LabelEncoder()\n",
    "y = class_le.fit_transform(df['classlabel'].values)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_le.inverse_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing one-hot encoding on nominal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['color', 'size', 'price']].values\n",
    "\n",
    "color_le = LabelEncoder()\n",
    "X[:, 0] = color_le.fit_transform(X[:, 0])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ohe = OneHotEncoder(categorical_features=[0])\n",
    "ohe.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([['green', 'M', 10.1, 'class1'],\n",
    "                   ['red', 'L', 13.5, 'class2'],\n",
    "                   ['blue', 'XL', 15.3, 'class1']])\n",
    "\n",
    "df.columns = ['color', 'size', 'price', 'classlabel']\n",
    "\n",
    "pd.get_dummies(df[['price', 'color', 'size']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define preprocessing pipeline with one hot encoder\n",
    "allows different transformations on various columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "df = pd.DataFrame([['green', 'M', 10.1, 'class1'],\n",
    "                   ['red', 'L', 13.5, 'class2'],\n",
    "                   ['blue', 'XL', 15.3, 'class1']])\n",
    "\n",
    "df.columns = ['color', 'size', 'price', 'classlabel']\n",
    "X = df[['color', 'size', 'price', 'classlabel']].values\n",
    "\n",
    "# perform one hot encoding on column 0, keep remaining columns un-touched\n",
    "preprocess = make_column_transformer((OneHotEncoder(), [0]), remainder='passthrough')\n",
    "print preprocess.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding using panda's get_dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([['green', 'M', 10.1, 'class1'],\n",
    "                   ['red', 'L', 13.5, 'class2'],\n",
    "                   ['blue', 'XL', 15.3, 'class1']])\n",
    "\n",
    "df.columns = ['color', 'size', 'price', 'classlabel']\n",
    "\n",
    "df = pd.concat([df, pd.get_dummies(df['color'].values, prefix='color')], axis=1)\n",
    "df.drop(['color'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reimport the Iris Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/'\n",
    "        'machine-learning-databases/iris/iris.data', header=None)\n",
    "X = df.iloc[0:100, [0, 1]].values\n",
    "y = df.iloc[0:100, 4].values\n",
    "y = np.where(y == 'Iris-setosa', 0, 1)\n",
    "\n",
    "print \"X:\", X[0:5]\n",
    "print \"y:\", y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the Training and Testing sets.\n",
    "\n",
    "In this case, we will use 60% for sample training and 40% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.2, random_state=5)\n",
    "\n",
    "print \"Training set samples: \", len(X_train)\n",
    "print \"Testing set samples: \", len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scales each column to have mean 0 and unit variance:\n",
    "\\begin{equation}\n",
    "{{\\mathbf{z}}_i} = \\frac{{{{\\mathbf{x}}_i} - {\\mu _i}}}{{{\\sigma _i}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "stdscaler = preprocessing.StandardScaler().fit(X_train)\n",
    "print \"Means of columns: \", stdscaler.mean_, \"\\nStandard deviation of columns:\", stdscaler.scale_\n",
    "\n",
    "X_scaled  = stdscaler.transform(X)\n",
    "X_train_scaled = stdscaler.transform(X_train)\n",
    "X_test_scaled  = stdscaler.transform(X_test)\n",
    "\n",
    "print \"X unscaled: \", X[0:5]\n",
    "print \"X scaled:   \", X_scaled[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaline Model in Scikit-learn with Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "ppn = Perceptron(eta0=0.01,n_iter=5)\n",
    "ppn.fit(X_train, y_train)\n",
    "predicted = ppn.predict(X_test)\n",
    "simplemetrics(y_test,predicted)\n",
    "plot_decision_2d(ppn,X,y,title=\"Full Data Set\")\n",
    "plot_decision_2d(ppn,X_train,y_train,title=\"Training Set\")\n",
    "plot_decision_2d(ppn,X_test,y_test,title=\"Testing Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "ppn = Perceptron(eta0=0.01,n_iter=5)\n",
    "ppn.fit(X_train_scaled, y_train)\n",
    "predicted = ppn.predict(X_test_scaled)\n",
    "simplemetrics(y_test,predicted)\n",
    "plot_decision_2d(ppn,X_scaled,y,title=\"Full Data Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the notebook, implement Gaussian Naive Bayes using the documentation found here:<br>\n",
    "http://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes\n",
    "\n",
    "Modify the cell below to use Gaussian Naive Bayes instead of a Perceptron. Make sure it is displaying the results for a Gaussian Naive Bayes model, including plotting the decision region. The results should look sililar to the image below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('week2result.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Implement Your Code Here:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't need to modify this code.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "from mlclass import simplemetrics, plot_decision_2d\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/'\n",
    "        'machine-learning-databases/iris/iris.data', header=None)\n",
    "X = df.iloc[0:100, [0, 1]].values\n",
    "y = df.iloc[0:100, 4].values\n",
    "y = np.where(y == 'Iris-setosa', 0, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "    X, y, test_size=0.2, random_state=5)\n",
    "\n",
    "stdscaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_scaled  = stdscaler.transform(X)\n",
    "X_train_scaled = stdscaler.transform(X_train)\n",
    "X_test_scaled  = stdscaler.transform(X_test)\n",
    "\n",
    "# **** modify the code below to run a gnb model ****\n",
    "from sklearn.linear_model import Perceptron\n",
    "ppn = Perceptron(eta0=0.01,n_iter=5)\n",
    "ppn.fit(X_train_scaled, y_train)\n",
    "predicted = ppn.predict(X_test_scaled)\n",
    "simplemetrics(y_test,predicted)\n",
    "plot_decision_2d(ppn,X_scaled,y,title=\"Full Data Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Install TextBlob into your Python distribution</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type the following into the command line (Command Line on Windows, Terminal on Mac/Linux) to install TextBlob (a natural language toolkit for Python):\n",
    "\n",
    "`pip install -U textblob`<br>\n",
    "`python -m textblob.download_corpora`\n",
    "\n",
    "After textblob is installed, run the following code. It should output the following:<br>\n",
    "`Sentiment(polarity=0.39166666666666666, subjectivity=0.4357142857142857)\n",
    "Sentiment(polarity=-0.4, subjectivity=0.825)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "testimonial = TextBlob(\"Textblob is amazingly simple to use. What great fun!\")\n",
    "print testimonial.sentiment\n",
    "\n",
    "testimonial = TextBlob(\"Textblob is annoying. What total junk.\")\n",
    "print testimonial.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use TextBlob next week to build a spam filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
